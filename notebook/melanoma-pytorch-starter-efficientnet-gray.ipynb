{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install efficientnet_pytorch torchtoolbox\n!pip install chart_studio","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-07T00:20:26.222824Z","iopub.execute_input":"2021-06-07T00:20:26.223072Z","iopub.status.idle":"2021-06-07T00:20:42.260371Z","shell.execute_reply.started":"2021-06-07T00:20:26.223047Z","shell.execute_reply":"2021-06-07T00:20:42.259485Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting efficientnet_pytorch\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\nCollecting torchtoolbox\n  Downloading torchtoolbox-0.1.5-py3-none-any.whl (58 kB)\n\u001b[K     |████████████████████████████████| 58 kB 881 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet_pytorch) (1.5.0)\nCollecting lmdb\n  Downloading lmdb-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (299 kB)\n\u001b[K     |████████████████████████████████| 299 kB 1.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.18.1)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (4.2.0.34)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (0.16.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.14.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.4.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (0.22.2.post1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (4.45.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (0.18.2)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torchtoolbox) (0.14.1)\nBuilding wheels for collected packages: efficientnet-pytorch\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=906316986454dec7809365b65220914aebfc33ce565e0ff2b13dbda85579b076\n  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\nSuccessfully built efficientnet-pytorch\nInstalling collected packages: efficientnet-pytorch, lmdb, torchtoolbox\nSuccessfully installed efficientnet-pytorch-0.7.1 lmdb-1.2.1 torchtoolbox-0.1.5\n\u001b[33mWARNING: You are using pip version 20.1; however, version 21.1.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nCollecting chart_studio\n  Downloading chart_studio-1.1.0-py3-none-any.whl (64 kB)\n\u001b[K     |████████████████████████████████| 64 kB 727 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from chart_studio) (2.23.0)\nRequirement already satisfied: plotly in /opt/conda/lib/python3.7/site-packages (from chart_studio) (4.7.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from chart_studio) (1.14.0)\nRequirement already satisfied: retrying>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from chart_studio) (1.3.3)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->chart_studio) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->chart_studio) (2.9)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->chart_studio) (1.24.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->chart_studio) (2020.4.5.1)\nInstalling collected packages: chart-studio\nSuccessfully installed chart-studio-1.1.0\n\u001b[33mWARNING: You are using pip version 20.1; however, version 21.1.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torchtoolbox.transform as transforms\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nimport pandas as pd\nimport numpy as np\nimport gc\nimport os\nimport cv2\nimport time\nimport datetime\nimport warnings\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom efficientnet_pytorch import EfficientNet\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-07T00:20:42.264560Z","iopub.execute_input":"2021-06-07T00:20:42.264838Z","iopub.status.idle":"2021-06-07T00:20:44.943126Z","shell.execute_reply.started":"2021-06-07T00:20:42.264810Z","shell.execute_reply":"2021-06-07T00:20:44.942363Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"warnings.simplefilter('ignore')\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(47)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T00:20:44.946440Z","iopub.execute_input":"2021-06-07T00:20:44.946702Z","iopub.status.idle":"2021-06-07T00:20:44.955297Z","shell.execute_reply.started":"2021-06-07T00:20:44.946669Z","shell.execute_reply":"2021-06-07T00:20:44.954637Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2021-06-07T00:20:44.956872Z","iopub.execute_input":"2021-06-07T00:20:44.957227Z","iopub.status.idle":"2021-06-07T00:20:44.983906Z","shell.execute_reply.started":"2021-06-07T00:20:44.957191Z","shell.execute_reply":"2021-06-07T00:20:44.983198Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class MelanomaDataset(Dataset):\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        imfolder: str,\n        train: bool = True,\n        transforms=None,\n        meta_features=None,\n    ):\n        self.df = df\n        self.imfolder = imfolder\n        self.transforms = transforms\n        self.train = train\n        self.meta_features = meta_features\n\n    def __getitem__(self, index):\n        im_path = os.path.join(\n            self.imfolder, self.df.iloc[index][\"image_name\"] + \".jpg\"\n        )\n        x = cv2.imread(im_path)\n        x = cv2.cvtColor(x, cv2.COLOR_RGB2GRAY)\n        \n        meta = np.array(\n            self.df.iloc[index][self.meta_features].values, dtype=np.float32\n        )\n\n        if self.transforms:\n            x = self.transforms(x)\n\n        if self.train:\n            y = self.df.iloc[index][\"target\"]\n            return (x, meta), y\n        else:\n            return (x, meta)\n\n    def __len__(self):\n        return len(self.df)\n\n\nclass Net(nn.Module):\n    def __init__(self, arch, n_meta_features: int):\n        super(Net, self).__init__()\n        self.arch = arch\n        if \"ResNet\" in str(arch.__class__):\n            self.arch.fc = nn.Linear(in_features=512, out_features=500, bias=True)\n        if \"EfficientNet\" in str(arch.__class__):\n            self.arch._fc = nn.Linear(in_features=1280, out_features=500, bias=True)\n        self.meta = nn.Sequential(\n            nn.Linear(n_meta_features, 500),\n            nn.BatchNorm1d(500),\n            nn.ReLU(),\n            nn.Dropout(p=0.2),\n            nn.Linear(500, 250),\n            nn.BatchNorm1d(250),\n            nn.ReLU(),\n            nn.Dropout(p=0.2),\n        )\n        self.ouput = nn.Linear(500 + 250, 1)\n\n    def forward(self, inputs):\n        x, meta = inputs\n        cnn_features = self.arch(x)\n        meta_features = self.meta(meta)\n        features = torch.cat((cnn_features, meta_features), dim=1)\n        output = self.ouput(features)\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2021-06-07T00:20:44.988667Z","iopub.execute_input":"2021-06-07T00:20:44.988930Z","iopub.status.idle":"2021-06-07T00:20:45.007941Z","shell.execute_reply.started":"2021-06-07T00:20:44.988906Z","shell.execute_reply":"2021-06-07T00:20:45.007117Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class AdvancedHairAugmentation:\n    def __init__(self, hairs: int = 5, hairs_folder: str = \"\"):\n        self.hairs = hairs\n        self.hairs_folder = hairs_folder\n\n    def __call__(self, img):\n        n_hairs = random.randint(0, self.hairs)\n\n        if not n_hairs:\n            return img\n\n        height, width = img.shape  # target image width and height\n        hair_images = [im for im in os.listdir(self.hairs_folder) if \"png\" in im]\n\n        for _ in range(n_hairs):\n            hair = cv2.imread(\n                os.path.join(self.hairs_folder, random.choice(hair_images))\n            )\n            hair = cv2.cvtColor(hair, cv2.COLOR_BGR2GRAY)\n            hair = cv2.flip(hair, random.choice([-1, 0, 1]))\n            hair = cv2.rotate(hair, random.choice([0, 1, 2]))\n\n            h_height, h_width = hair.shape  # hair image width and height\n            roi_ho = random.randint(0, img.shape[0] - hair.shape[0])\n            roi_wo = random.randint(0, img.shape[1] - hair.shape[1])\n            roi = img[roi_ho : roi_ho + h_height, roi_wo : roi_wo + h_width]\n\n            # Creating a mask and inverse mask\n            #img2gray = cv2.cvtColor(hair, cv2.COLOR_BGR2GRAY)\n            #ret, mask = cv2.threshold(img2gray, 10, 255, cv2.THRESH_BINARY)\n            ret, mask = cv2.threshold(hair, 10, 255, cv2.THRESH_BINARY)\n            mask_inv = cv2.bitwise_not(mask)\n\n            # Now black-out the area of hair in ROI\n            img_bg = cv2.bitwise_and(roi, roi, mask=mask_inv)\n\n            # Take only region of hair from hair image.\n            hair_fg = cv2.bitwise_and(hair, hair, mask=mask)\n\n            # Put hair in ROI and modify the target image\n            dst = cv2.add(img_bg, hair_fg)\n\n            img[roi_ho : roi_ho + h_height, roi_wo : roi_wo + h_width] = dst\n\n        return img\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}(hairs={self.hairs}, hairs_folder=\"{self.hairs_folder}\")'\n","metadata":{"execution":{"iopub.status.busy":"2021-06-07T00:20:45.010407Z","iopub.execute_input":"2021-06-07T00:20:45.010748Z","iopub.status.idle":"2021-06-07T00:20:45.027393Z","shell.execute_reply.started":"2021-06-07T00:20:45.010711Z","shell.execute_reply":"2021-06-07T00:20:45.026573Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class DrawHair:\n    def __init__(self, hairs: int = 4, width: tuple = (1, 2)):\n        self.hairs = hairs\n        self.width = width\n\n    def __call__(self, img):\n        if not self.hairs:\n            return img\n\n        width, height, _ = img.shape\n\n        for _ in range(random.randint(0, self.hairs)):\n            # The origin point of the line will always be at the top half of the image\n            origin = (random.randint(0, width), random.randint(0, height // 2))\n            # The end of the line\n            end = (random.randint(0, width), random.randint(0, height))\n            color = (0, 0, 0)  # color of the hair. Black.\n            cv2.line(\n                img, origin, end, color, random.randint(self.width[0], self.width[1])\n            )\n\n        return img\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}(hairs={self.hairs}, width={self.width})\"","metadata":{"execution":{"iopub.status.busy":"2021-06-07T00:20:45.028913Z","iopub.execute_input":"2021-06-07T00:20:45.029306Z","iopub.status.idle":"2021-06-07T00:20:45.040552Z","shell.execute_reply.started":"2021-06-07T00:20:45.029268Z","shell.execute_reply":"2021-06-07T00:20:45.039529Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Microscope:\n    def __init__(self, p: float = 0.5):\n        self.p = p\n\n    def __call__(self, img):\n        if random.random() < self.p:\n            circle = cv2.circle(\n                (np.ones(img.shape) * 255).astype(np.uint8),  # image placeholder\n                (img.shape[0] // 2, img.shape[1] // 2),  # center point of circle\n                random.randint(img.shape[0] // 2 - 3, img.shape[0] // 2 + 15),  # radius\n                (0, 0, 0),  # color\n                -1,\n            )\n\n            mask = circle - 255\n            img = np.multiply(img, mask)\n\n        return img\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}(p={self.p})\"","metadata":{"execution":{"iopub.status.busy":"2021-06-07T00:20:45.041959Z","iopub.execute_input":"2021-06-07T00:20:45.042414Z","iopub.status.idle":"2021-06-07T00:20:45.053277Z","shell.execute_reply.started":"2021-06-07T00:20:45.042362Z","shell.execute_reply":"2021-06-07T00:20:45.052318Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    AdvancedHairAugmentation(hairs_folder='/kaggle/input/melanoma-hairs'),\n    transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    Microscope(p=0.5),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n])\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2021-06-07T00:20:45.054527Z","iopub.execute_input":"2021-06-07T00:20:45.054895Z","iopub.status.idle":"2021-06-07T00:20:45.063380Z","shell.execute_reply.started":"2021-06-07T00:20:45.054859Z","shell.execute_reply":"2021-06-07T00:20:45.062365Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"arch = EfficientNet.from_pretrained('efficientnet-b1')","metadata":{"execution":{"iopub.status.busy":"2021-06-07T00:20:45.064770Z","iopub.execute_input":"2021-06-07T00:20:45.065201Z","iopub.status.idle":"2021-06-07T00:20:46.334111Z","shell.execute_reply.started":"2021-06-07T00:20:45.065147Z","shell.execute_reply":"2021-06-07T00:20:46.333282Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b1-f1951068.pth\" to /root/.cache/torch/checkpoints/efficientnet-b1-f1951068.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=31519111.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a02237a1e1a4cd2b322a27391fcbd3c"}},"metadata":{}},{"name":"stdout","text":"\nLoaded pretrained weights for efficientnet-b1\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/jpeg-melanoma-256x256/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-07T00:20:46.335524Z","iopub.execute_input":"2021-06-07T00:20:46.335878Z","iopub.status.idle":"2021-06-07T00:20:46.418467Z","shell.execute_reply.started":"2021-06-07T00:20:46.335839Z","shell.execute_reply":"2021-06-07T00:20:46.417751Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# One-hot encoding of anatom_site_general_challenge feature\ndummies = pd.get_dummies(train_df['anatom_site_general_challenge'], dummy_na=True, dtype=np.uint8, prefix='site')\ntrain_df = pd.concat([train_df, dummies.iloc[:train_df.shape[0]]], axis=1)\n\n# Sex features\ntrain_df['sex'] = train_df['sex'].map({'male': 1, 'female': 0})\ntrain_df['sex'] = train_df['sex'].fillna(-1)\n\n# Age features\ntrain_df['age_approx'] /= train_df['age_approx'].max()\ntrain_df['age_approx'] = train_df['age_approx'].fillna(0)\ntrain_df['patient_id'] = train_df['patient_id'].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T00:20:46.419805Z","iopub.execute_input":"2021-06-07T00:20:46.420346Z","iopub.status.idle":"2021-06-07T00:20:46.468180Z","shell.execute_reply.started":"2021-06-07T00:20:46.420306Z","shell.execute_reply":"2021-06-07T00:20:46.467482Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"meta_features = ['sex', 'age_approx'] + [col for col in train_df.columns if 'site_' in col]\nmeta_features.remove('anatom_site_general_challenge')","metadata":{"execution":{"iopub.status.busy":"2021-06-07T00:20:46.470086Z","iopub.execute_input":"2021-06-07T00:20:46.470396Z","iopub.status.idle":"2021-06-07T00:20:46.475805Z","shell.execute_reply.started":"2021-06-07T00:20:46.470369Z","shell.execute_reply":"2021-06-07T00:20:46.474910Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = train_df.drop(\"target\", axis=1)\ny = train_df[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T00:20:46.477382Z","iopub.execute_input":"2021-06-07T00:20:46.477945Z","iopub.status.idle":"2021-06-07T00:20:46.498259Z","shell.execute_reply.started":"2021-06-07T00:20:46.477906Z","shell.execute_reply":"2021-06-07T00:20:46.497553Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"X_train[\"target\"] = y_train","metadata":{"execution":{"iopub.status.busy":"2021-06-07T00:20:46.499650Z","iopub.execute_input":"2021-06-07T00:20:46.500019Z","iopub.status.idle":"2021-06-07T00:20:46.505876Z","shell.execute_reply.started":"2021-06-07T00:20:46.499983Z","shell.execute_reply":"2021-06-07T00:20:46.504783Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"test = MelanomaDataset(\n    df=X_test.reset_index(drop=True),\n    imfolder=\"/kaggle/input/melanoma-external-malignant-256/train/train/\",\n    train=False,\n    transforms=train_transform,  # For TTA\n    meta_features=meta_features,\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T00:20:46.507308Z","iopub.execute_input":"2021-06-07T00:20:46.507704Z","iopub.status.idle":"2021-06-07T00:20:46.515343Z","shell.execute_reply.started":"2021-06-07T00:20:46.507655Z","shell.execute_reply":"2021-06-07T00:20:46.514592Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"skf = GroupKFold(n_splits=5)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T00:20:46.516638Z","iopub.execute_input":"2021-06-07T00:20:46.517047Z","iopub.status.idle":"2021-06-07T00:20:46.524503Z","shell.execute_reply.started":"2021-06-07T00:20:46.517008Z","shell.execute_reply":"2021-06-07T00:20:46.523559Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"epochs = 5  # Number of epochs to run\nes_patience = (\n    3  # Early Stopping patience - for how many epochs with no improvements to wait\n)\nTTA = 3  # Test Time Augmentation rounds\n\noof = np.zeros((len(X_train), 1))  # Out Of Fold predictions\npreds = torch.zeros(\n    (len(test), 1), dtype=torch.float32, device=device\n)  # Predictions for test test\n\n# skf = KFold(n_splits=5, shuffle=True, random_state=47)\nfor fold, (train_idx, val_idx) in enumerate(\n    skf.split(\n        X=np.zeros(len(X_train)),\n        y=X_train[\"target\"],\n        groups=X_train[\"patient_id\"].tolist(),\n    ),\n    1,\n):\n    print(\"=\" * 20, \"Fold\", fold, \"=\" * 20)\n\n    model_path = f\"model_{fold}.pth\"  # Path and filename to save model to\n    best_val = 0  # Best validation score within this fold\n    patience = es_patience  # Current patience counter\n    arch = EfficientNet.from_pretrained(\"efficientnet-b1\")\n    model = Net(\n        arch=arch, n_meta_features=len(meta_features)\n    )  # New model for each fold\n    model = model.to(device)\n\n    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n    scheduler = ReduceLROnPlateau(\n        optimizer=optim, mode=\"max\", patience=1, verbose=True, factor=0.2\n    )\n    criterion = nn.BCEWithLogitsLoss()\n\n    train = MelanomaDataset(\n        df=X_train.iloc[train_idx].reset_index(drop=True),\n        imfolder=\"/kaggle/input/melanoma-external-malignant-256/train/train/\",\n        train=True,\n        transforms=train_transform,\n        meta_features=meta_features,\n    )\n    val = MelanomaDataset(\n        df=X_train.iloc[val_idx].reset_index(drop=True),\n        imfolder=\"/kaggle/input/melanoma-external-malignant-256/train/train/\",\n        train=True,\n        transforms=test_transform,\n        meta_features=meta_features,\n    )\n\n    train_loader = DataLoader(dataset=train, batch_size=64, shuffle=True, num_workers=2)\n    val_loader = DataLoader(dataset=val, batch_size=16, shuffle=False, num_workers=2)\n    test_loader = DataLoader(dataset=test, batch_size=16, shuffle=False, num_workers=2)\n\n    for epoch in range(epochs):\n        start_time = time.time()\n        correct = 0\n        epoch_loss = 0\n        model.train()\n\n        for x, y in train_loader:\n            x[0] = torch.tensor(x[0], device=device, dtype=torch.float32)\n            x[1] = torch.tensor(x[1], device=device, dtype=torch.float32)\n            y = torch.tensor(y, device=device, dtype=torch.float32)\n            optim.zero_grad()\n            z = model(x)\n            loss = criterion(z, y.unsqueeze(1))\n            loss.backward()\n            optim.step()\n            pred = torch.round(\n                torch.sigmoid(z)\n            )  # round off sigmoid to obtain predictions\n            correct += (\n                (pred.cpu() == y.cpu().unsqueeze(1)).sum().item()\n            )  # tracking number of correctly predicted samples\n            epoch_loss += loss.item()\n        train_acc = correct / len(train_idx)\n\n        model.eval()  # switch model to the evaluation mode\n        val_preds = torch.zeros((len(val_idx), 1), dtype=torch.float32, device=device)\n        with torch.no_grad():  # Do not calculate gradient since we are only predicting\n            # Predicting on validation set\n            for j, (x_val, y_val) in enumerate(val_loader):\n                x_val[0] = torch.tensor(x_val[0], device=device, dtype=torch.float32)\n                x_val[1] = torch.tensor(x_val[1], device=device, dtype=torch.float32)\n                y_val = torch.tensor(y_val, device=device, dtype=torch.float32)\n                z_val = model(x_val)\n                val_pred = torch.sigmoid(z_val)\n                val_preds[\n                    j * val_loader.batch_size : j * val_loader.batch_size\n                    + x_val[0].shape[0]\n                ] = val_pred\n            val_acc = accuracy_score(\n                X_train.iloc[val_idx][\"target\"].values, torch.round(val_preds.cpu())\n            )\n            val_roc = roc_auc_score(\n                X_train.iloc[val_idx][\"target\"].values, val_preds.cpu()\n            )\n\n            print(\n                \"Epoch {:03}: | Loss: {:.3f} | Train acc: {:.3f} | Val acc: {:.3f} | Val roc_auc: {:.3f} | Training time: {}\".format(\n                    epoch + 1,\n                    epoch_loss,\n                    train_acc,\n                    val_acc,\n                    val_roc,\n                    str(datetime.timedelta(seconds=time.time() - start_time))[:7],\n                )\n            )\n\n            scheduler.step(val_roc)\n\n            if val_roc >= best_val:\n                best_val = val_roc\n                patience = es_patience\n                torch.save(model, model_path)\n            else:\n                patience -= 1\n                if patience == 0:\n                    print(\"Early stopping. Best Val roc_auc: {:.3f}\".format(best_val))\n                    break\n\n    model = torch.load(model_path)\n    model.eval()\n    val_preds = torch.zeros((len(val_idx), 1), dtype=torch.float32, device=device)\n\n    with torch.no_grad():\n        for j, (x_val, y_val) in enumerate(val_loader):\n            x_val[0] = torch.tensor(x_val[0], device=device, dtype=torch.float32)\n            x_val[1] = torch.tensor(x_val[1], device=device, dtype=torch.float32)\n            y_val = torch.tensor(y_val, device=device, dtype=torch.float32)\n            z_val = model(x_val)\n            val_pred = torch.sigmoid(z_val)\n            val_preds[\n                j * val_loader.batch_size : j * val_loader.batch_size\n                + x_val[0].shape[0]\n            ] = val_pred\n        oof[val_idx] = val_preds.cpu().numpy()\n\n        tta_preds = torch.zeros((len(test), 1), dtype=torch.float32, device=device)\n        for _ in range(TTA):\n            for i, x_test in enumerate(test_loader):\n                x_test[0] = torch.tensor(x_test[0], device=device, dtype=torch.float32)\n                x_test[1] = torch.tensor(x_test[1], device=device, dtype=torch.float32)\n                z_test = model(x_test)\n                z_test = torch.sigmoid(z_test)\n                tta_preds[\n                    i * test_loader.batch_size : i * test_loader.batch_size\n                    + x_test[0].shape[0]\n                ] += z_test\n        preds += tta_preds / TTA\n\npreds /= skf.n_splits","metadata":{"execution":{"iopub.status.busy":"2021-06-07T00:20:46.525820Z","iopub.execute_input":"2021-06-07T00:20:46.526291Z","iopub.status.idle":"2021-06-07T02:24:09.538809Z","shell.execute_reply.started":"2021-06-07T00:20:46.526251Z","shell.execute_reply":"2021-06-07T02:24:09.536614Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"==================== Fold 1 ====================\nLoaded pretrained weights for efficientnet-b1\nEpoch 001: | Loss: 29.230 | Train acc: 0.982 | Val acc: 0.981 | Val roc_auc: 0.729 | Training time: 0:05:21\nEpoch 002: | Loss: 25.401 | Train acc: 0.983 | Val acc: 0.981 | Val roc_auc: 0.835 | Training time: 0:04:11\nEpoch 003: | Loss: 24.898 | Train acc: 0.983 | Val acc: 0.981 | Val roc_auc: 0.852 | Training time: 0:04:09\nEpoch 004: | Loss: 24.936 | Train acc: 0.983 | Val acc: 0.981 | Val roc_auc: 0.814 | Training time: 0:04:11\nEpoch 005: | Loss: 24.472 | Train acc: 0.983 | Val acc: 0.981 | Val roc_auc: 0.777 | Training time: 0:04:09\nEpoch     5: reducing learning rate of group 0 to 2.0000e-04.\n==================== Fold 2 ====================\nLoaded pretrained weights for efficientnet-b1\nEpoch 001: | Loss: 31.227 | Train acc: 0.979 | Val acc: 0.983 | Val roc_auc: 0.810 | Training time: 0:04:10\nEpoch 002: | Loss: 26.324 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.784 | Training time: 0:04:10\nEpoch 003: | Loss: 25.803 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.704 | Training time: 0:04:27\nEpoch     3: reducing learning rate of group 0 to 2.0000e-04.\nEpoch 004: | Loss: 24.099 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.876 | Training time: 0:04:10\nEpoch 005: | Loss: 23.282 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.877 | Training time: 0:04:10\n==================== Fold 3 ====================\nLoaded pretrained weights for efficientnet-b1\nEpoch 001: | Loss: 29.853 | Train acc: 0.980 | Val acc: 0.983 | Val roc_auc: 0.781 | Training time: 0:04:11\nEpoch 002: | Loss: 26.062 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.758 | Training time: 0:04:12\nEpoch 003: | Loss: 25.947 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.815 | Training time: 0:04:13\nEpoch 004: | Loss: 25.606 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.730 | Training time: 0:04:11\nEpoch 005: | Loss: 25.340 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.754 | Training time: 0:04:11\nEpoch     5: reducing learning rate of group 0 to 2.0000e-04.\n==================== Fold 4 ====================\nLoaded pretrained weights for efficientnet-b1\nEpoch 001: | Loss: 30.641 | Train acc: 0.980 | Val acc: 0.983 | Val roc_auc: 0.823 | Training time: 0:04:11\nEpoch 002: | Loss: 26.326 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.812 | Training time: 0:04:12\nEpoch 003: | Loss: 25.707 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.840 | Training time: 0:04:13\nEpoch 004: | Loss: 26.253 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.783 | Training time: 0:04:11\nEpoch 005: | Loss: 25.574 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.855 | Training time: 0:04:11\n==================== Fold 5 ====================\nLoaded pretrained weights for efficientnet-b1\nEpoch 001: | Loss: 30.629 | Train acc: 0.980 | Val acc: 0.983 | Val roc_auc: 0.831 | Training time: 0:04:11\nEpoch 002: | Loss: 25.412 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.803 | Training time: 0:04:11\nEpoch 003: | Loss: 24.779 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.817 | Training time: 0:04:12\nEpoch     3: reducing learning rate of group 0 to 2.0000e-04.\nEpoch 004: | Loss: 23.137 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.858 | Training time: 0:04:10\nEpoch 005: | Loss: 22.560 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.863 | Training time: 0:04:13\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f'OOF: {roc_auc_score(y_train, oof):.5f}')\nprint(f\"Test Score: {roc_auc_score(y_test.values, preds.cpu().numpy().flatten()):.5f}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-07T02:24:09.541477Z","iopub.execute_input":"2021-06-07T02:24:09.541751Z","iopub.status.idle":"2021-06-07T02:24:09.577526Z","shell.execute_reply.started":"2021-06-07T02:24:09.541722Z","shell.execute_reply":"2021-06-07T02:24:09.576707Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"OOF: 0.84731\nTest Score: 0.85640\n","output_type":"stream"}]},{"cell_type":"code","source":"sns.kdeplot(pd.Series(preds.cpu().numpy().reshape(-1,)));","metadata":{"execution":{"iopub.status.busy":"2021-06-07T02:24:09.581930Z","iopub.execute_input":"2021-06-07T02:24:09.585444Z","iopub.status.idle":"2021-06-07T02:24:09.834113Z","shell.execute_reply.started":"2021-06-07T02:24:09.585405Z","shell.execute_reply":"2021-06-07T02:24:09.833123Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"# Saving OOF predictions so stacking would be easier\npd.Series(oof.reshape(-1,)).to_csv('oof.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T02:24:09.837485Z","iopub.execute_input":"2021-06-07T02:24:09.839987Z","iopub.status.idle":"2021-06-07T02:24:10.244582Z","shell.execute_reply.started":"2021-06-07T02:24:09.839946Z","shell.execute_reply":"2021-06-07T02:24:10.243595Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame(X_test[\"image_name\"])\nsub['target'] = preds.cpu().numpy().reshape(-1,)\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T02:24:10.248941Z","iopub.execute_input":"2021-06-07T02:24:10.251905Z","iopub.status.idle":"2021-06-07T02:24:10.302602Z","shell.execute_reply.started":"2021-06-07T02:24:10.251862Z","shell.execute_reply":"2021-06-07T02:24:10.301648Z"},"trusted":true},"execution_count":22,"outputs":[]}]}