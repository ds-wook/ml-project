{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-06-01T06:45:20.495522Z",
     "iopub.status.busy": "2021-06-01T06:45:20.495039Z",
     "iopub.status.idle": "2021-06-01T06:45:31.289417Z",
     "shell.execute_reply": "2021-06-01T06:45:31.288547Z",
     "shell.execute_reply.started": "2021-06-01T06:45:20.495485Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: efficientnet_pytorch in /opt/conda/lib/python3.7/site-packages (0.7.1)\n",
      "Requirement already satisfied: torchtoolbox in /opt/conda/lib/python3.7/site-packages (0.1.5)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet_pytorch) (1.5.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (4.45.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.14.0)\n",
      "Requirement already satisfied: lmdb in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.2.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.4.1)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (0.22.2.post1)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (4.2.0.34)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.18.1)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (0.18.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torchtoolbox) (0.14.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: chart_studio in /opt/conda/lib/python3.7/site-packages (1.1.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from chart_studio) (1.3.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from chart_studio) (2.23.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from chart_studio) (1.14.0)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.7/site-packages (from chart_studio) (4.7.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->chart_studio) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->chart_studio) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->chart_studio) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->chart_studio) (3.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet_pytorch torchtoolbox\n",
    "!pip install chart_studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-01T06:45:31.291354Z",
     "iopub.status.busy": "2021-06-01T06:45:31.290968Z",
     "iopub.status.idle": "2021-06-01T06:45:32.226890Z",
     "shell.execute_reply": "2021-06-01T06:45:32.226107Z",
     "shell.execute_reply.started": "2021-06-01T06:45:31.291308Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchtoolbox.transform as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T06:45:32.228530Z",
     "iopub.status.busy": "2021-06-01T06:45:32.228145Z",
     "iopub.status.idle": "2021-06-01T06:45:32.237677Z",
     "shell.execute_reply": "2021-06-01T06:45:32.236669Z",
     "shell.execute_reply.started": "2021-06-01T06:45:32.228476Z"
    }
   },
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything(47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T06:45:32.239862Z",
     "iopub.status.busy": "2021-06-01T06:45:32.239338Z",
     "iopub.status.idle": "2021-06-01T06:45:32.275084Z",
     "shell.execute_reply": "2021-06-01T06:45:32.274355Z",
     "shell.execute_reply.started": "2021-06-01T06:45:32.239821Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T06:45:32.278762Z",
     "iopub.status.busy": "2021-06-01T06:45:32.278479Z",
     "iopub.status.idle": "2021-06-01T06:45:32.299700Z",
     "shell.execute_reply": "2021-06-01T06:45:32.298861Z",
     "shell.execute_reply.started": "2021-06-01T06:45:32.278734Z"
    }
   },
   "outputs": [],
   "source": [
    "class MelanomaDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        imfolder: str,\n",
    "        train: bool = True,\n",
    "        transforms=None,\n",
    "        meta_features=None,\n",
    "    ):\n",
    "        self.df = df\n",
    "        self.imfolder = imfolder\n",
    "        self.transforms = transforms\n",
    "        self.train = train\n",
    "        self.meta_features = meta_features\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        im_path = os.path.join(\n",
    "            self.imfolder, self.df.iloc[index][\"image_name\"] + \".jpg\"\n",
    "        )\n",
    "        x = cv2.imread(im_path)\n",
    "        meta = np.array(\n",
    "            self.df.iloc[index][self.meta_features].values, dtype=np.float32\n",
    "        )\n",
    "\n",
    "        if self.transforms:\n",
    "            x = self.transforms(x)\n",
    "\n",
    "        if self.train:\n",
    "            y = self.df.iloc[index][\"target\"]\n",
    "            return (x, meta), y\n",
    "        else:\n",
    "            return (x, meta)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, arch, n_meta_features: int):\n",
    "        super(Net, self).__init__()\n",
    "        self.arch = arch\n",
    "        if \"ResNet\" in str(arch.__class__):\n",
    "            self.arch.fc = nn.Linear(in_features=512, out_features=500, bias=True)\n",
    "        if \"EfficientNet\" in str(arch.__class__):\n",
    "            self.arch._fc = nn.Linear(in_features=1280, out_features=500, bias=True)\n",
    "        self.meta = nn.Sequential(\n",
    "            nn.Linear(n_meta_features, 500),\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(500, 250),\n",
    "            nn.BatchNorm1d(250),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "        )\n",
    "        self.ouput = nn.Linear(500 + 250, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x, meta = inputs\n",
    "        cnn_features = self.arch(x)\n",
    "        meta_features = self.meta(meta)\n",
    "        features = torch.cat((cnn_features, meta_features), dim=1)\n",
    "        output = self.ouput(features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T06:45:32.304197Z",
     "iopub.status.busy": "2021-06-01T06:45:32.303916Z",
     "iopub.status.idle": "2021-06-01T06:45:32.320052Z",
     "shell.execute_reply": "2021-06-01T06:45:32.319227Z",
     "shell.execute_reply.started": "2021-06-01T06:45:32.304171Z"
    }
   },
   "outputs": [],
   "source": [
    "class AdvancedHairAugmentation:\n",
    "    def __init__(self, hairs: int = 5, hairs_folder: str = \"\"):\n",
    "        self.hairs = hairs\n",
    "        self.hairs_folder = hairs_folder\n",
    "\n",
    "    def __call__(self, img):\n",
    "        n_hairs = random.randint(0, self.hairs)\n",
    "\n",
    "        if not n_hairs:\n",
    "            return img\n",
    "\n",
    "        height, width, _ = img.shape  # target image width and height\n",
    "        hair_images = [im for im in os.listdir(self.hairs_folder) if \"png\" in im]\n",
    "\n",
    "        for _ in range(n_hairs):\n",
    "            hair = cv2.imread(\n",
    "                os.path.join(self.hairs_folder, random.choice(hair_images))\n",
    "            )\n",
    "            hair = cv2.flip(hair, random.choice([-1, 0, 1]))\n",
    "            hair = cv2.rotate(hair, random.choice([0, 1, 2]))\n",
    "\n",
    "            h_height, h_width, _ = hair.shape  # hair image width and height\n",
    "            roi_ho = random.randint(0, img.shape[0] - hair.shape[0])\n",
    "            roi_wo = random.randint(0, img.shape[1] - hair.shape[1])\n",
    "            roi = img[roi_ho : roi_ho + h_height, roi_wo : roi_wo + h_width]\n",
    "\n",
    "            # Creating a mask and inverse mask\n",
    "            img2gray = cv2.cvtColor(hair, cv2.COLOR_BGR2GRAY)\n",
    "            ret, mask = cv2.threshold(img2gray, 10, 255, cv2.THRESH_BINARY)\n",
    "            mask_inv = cv2.bitwise_not(mask)\n",
    "\n",
    "            # Now black-out the area of hair in ROI\n",
    "            img_bg = cv2.bitwise_and(roi, roi, mask=mask_inv)\n",
    "\n",
    "            # Take only region of hair from hair image.\n",
    "            hair_fg = cv2.bitwise_and(hair, hair, mask=mask)\n",
    "\n",
    "            # Put hair in ROI and modify the target image\n",
    "            dst = cv2.add(img_bg, hair_fg)\n",
    "\n",
    "            img[roi_ho : roi_ho + h_height, roi_wo : roi_wo + h_width] = dst\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}(hairs={self.hairs}, hairs_folder=\"{self.hairs_folder}\")'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T06:45:32.322573Z",
     "iopub.status.busy": "2021-06-01T06:45:32.322001Z",
     "iopub.status.idle": "2021-06-01T06:45:32.334793Z",
     "shell.execute_reply": "2021-06-01T06:45:32.333937Z",
     "shell.execute_reply.started": "2021-06-01T06:45:32.322525Z"
    }
   },
   "outputs": [],
   "source": [
    "class DrawHair:\n",
    "    def __init__(self, hairs: int = 4, width: tuple = (1, 2)):\n",
    "        self.hairs = hairs\n",
    "        self.width = width\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if not self.hairs:\n",
    "            return img\n",
    "\n",
    "        width, height, _ = img.shape\n",
    "\n",
    "        for _ in range(random.randint(0, self.hairs)):\n",
    "            # The origin point of the line will always be at the top half of the image\n",
    "            origin = (random.randint(0, width), random.randint(0, height // 2))\n",
    "            # The end of the line\n",
    "            end = (random.randint(0, width), random.randint(0, height))\n",
    "            color = (0, 0, 0)  # color of the hair. Black.\n",
    "            cv2.line(\n",
    "                img, origin, end, color, random.randint(self.width[0], self.width[1])\n",
    "            )\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(hairs={self.hairs}, width={self.width})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T06:45:32.336856Z",
     "iopub.status.busy": "2021-06-01T06:45:32.336095Z",
     "iopub.status.idle": "2021-06-01T06:45:32.349719Z",
     "shell.execute_reply": "2021-06-01T06:45:32.348981Z",
     "shell.execute_reply.started": "2021-06-01T06:45:32.336793Z"
    }
   },
   "outputs": [],
   "source": [
    "class Microscope:\n",
    "    def __init__(self, p: float = 0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            circle = cv2.circle(\n",
    "                (np.ones(img.shape) * 255).astype(np.uint8),  # image placeholder\n",
    "                (img.shape[0] // 2, img.shape[1] // 2),  # center point of circle\n",
    "                random.randint(img.shape[0] // 2 - 3, img.shape[0] // 2 + 15),  # radius\n",
    "                (0, 0, 0),  # color\n",
    "                -1,\n",
    "            )\n",
    "\n",
    "            mask = circle - 255\n",
    "            img = np.multiply(img, mask)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(p={self.p})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T06:45:32.351799Z",
     "iopub.status.busy": "2021-06-01T06:45:32.351176Z",
     "iopub.status.idle": "2021-06-01T06:45:32.361423Z",
     "shell.execute_reply": "2021-06-01T06:45:32.360372Z",
     "shell.execute_reply.started": "2021-06-01T06:45:32.351759Z"
    }
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        AdvancedHairAugmentation(hairs_folder=\"/kaggle/input/melanoma-hairs\"),\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        Microscope(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T06:45:32.363216Z",
     "iopub.status.busy": "2021-06-01T06:45:32.362766Z",
     "iopub.status.idle": "2021-06-01T06:45:32.538063Z",
     "shell.execute_reply": "2021-06-01T06:45:32.537156Z",
     "shell.execute_reply.started": "2021-06-01T06:45:32.363174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b1\n"
     ]
    }
   ],
   "source": [
    "arch = EfficientNet.from_pretrained(\"efficientnet-b1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T06:45:32.539658Z",
     "iopub.status.busy": "2021-06-01T06:45:32.539310Z",
     "iopub.status.idle": "2021-06-01T06:45:32.595398Z",
     "shell.execute_reply": "2021-06-01T06:45:32.594599Z",
     "shell.execute_reply.started": "2021-06-01T06:45:32.539628Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/input/jpeg-melanoma-256x256/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T06:45:32.597110Z",
     "iopub.status.busy": "2021-06-01T06:45:32.596707Z",
     "iopub.status.idle": "2021-06-01T06:45:32.631237Z",
     "shell.execute_reply": "2021-06-01T06:45:32.630530Z",
     "shell.execute_reply.started": "2021-06-01T06:45:32.597068Z"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot encoding of anatom_site_general_challenge feature\n",
    "dummies = pd.get_dummies(\n",
    "    train_df[\"anatom_site_general_challenge\"],\n",
    "    dummy_na=True,\n",
    "    dtype=np.uint8,\n",
    "    prefix=\"site\",\n",
    ")\n",
    "train_df = pd.concat([train_df, dummies.iloc[: train_df.shape[0]]], axis=1)\n",
    "\n",
    "# Sex features\n",
    "train_df[\"sex\"] = train_df[\"sex\"].map({\"male\": 1, \"female\": 0})\n",
    "train_df[\"sex\"] = train_df[\"sex\"].fillna(-1)\n",
    "\n",
    "# Age features\n",
    "train_df[\"age_approx\"] /= train_df[\"age_approx\"].max()\n",
    "train_df[\"age_approx\"] = train_df[\"age_approx\"].fillna(0)\n",
    "train_df[\"patient_id\"] = train_df[\"patient_id\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T06:45:32.632820Z",
     "iopub.status.busy": "2021-06-01T06:45:32.632486Z",
     "iopub.status.idle": "2021-06-01T06:45:32.637228Z",
     "shell.execute_reply": "2021-06-01T06:45:32.636397Z",
     "shell.execute_reply.started": "2021-06-01T06:45:32.632791Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_features = [\"sex\", \"age_approx\"] + [\n",
    "    col for col in train_df.columns if \"site_\" in col\n",
    "]\n",
    "meta_features.remove(\"anatom_site_general_challenge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T06:45:32.639161Z",
     "iopub.status.busy": "2021-06-01T06:45:32.638623Z",
     "iopub.status.idle": "2021-06-01T06:45:32.660975Z",
     "shell.execute_reply": "2021-06-01T06:45:32.660235Z",
     "shell.execute_reply.started": "2021-06-01T06:45:32.639122Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_df.drop(\"target\", axis=1)\n",
    "y = train_df[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T06:45:32.664065Z",
     "iopub.status.busy": "2021-06-01T06:45:32.663819Z",
     "iopub.status.idle": "2021-06-01T06:45:32.668804Z",
     "shell.execute_reply": "2021-06-01T06:45:32.667976Z",
     "shell.execute_reply.started": "2021-06-01T06:45:32.664039Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train[\"target\"] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T06:45:32.670912Z",
     "iopub.status.busy": "2021-06-01T06:45:32.670282Z",
     "iopub.status.idle": "2021-06-01T06:45:32.679790Z",
     "shell.execute_reply": "2021-06-01T06:45:32.678810Z",
     "shell.execute_reply.started": "2021-06-01T06:45:32.670873Z"
    }
   },
   "outputs": [],
   "source": [
    "test = MelanomaDataset(\n",
    "    df=X_test.reset_index(drop=True),\n",
    "    imfolder=\"/kaggle/input/melanoma-external-malignant-256/train/train/\",\n",
    "    train=False,\n",
    "    transforms=train_transform,  # For TTA\n",
    "    meta_features=meta_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T06:45:32.682820Z",
     "iopub.status.busy": "2021-06-01T06:45:32.682438Z",
     "iopub.status.idle": "2021-06-01T06:45:32.689019Z",
     "shell.execute_reply": "2021-06-01T06:45:32.688070Z",
     "shell.execute_reply.started": "2021-06-01T06:45:32.682781Z"
    }
   },
   "outputs": [],
   "source": [
    "skf = GroupKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T06:45:32.690938Z",
     "iopub.status.busy": "2021-06-01T06:45:32.690554Z",
     "iopub.status.idle": "2021-06-01T10:36:31.606380Z",
     "shell.execute_reply": "2021-06-01T10:36:31.605394Z",
     "shell.execute_reply.started": "2021-06-01T06:45:32.690898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Fold 1 ====================\n",
      "Loaded pretrained weights for efficientnet-b1\n",
      "Epoch 001: | Loss: 28.794 | Train acc: 0.982 | Val acc: 0.981 | Val roc_auc: 0.758 | Training time: 0:04:25\n",
      "Epoch 002: | Loss: 24.972 | Train acc: 0.983 | Val acc: 0.981 | Val roc_auc: 0.820 | Training time: 0:04:21\n",
      "Epoch 003: | Loss: 24.701 | Train acc: 0.983 | Val acc: 0.981 | Val roc_auc: 0.807 | Training time: 0:04:23\n",
      "Epoch 004: | Loss: 24.355 | Train acc: 0.983 | Val acc: 0.981 | Val roc_auc: 0.866 | Training time: 0:04:22\n",
      "Epoch 005: | Loss: 24.327 | Train acc: 0.983 | Val acc: 0.981 | Val roc_auc: 0.822 | Training time: 0:04:23\n",
      "Epoch 006: | Loss: 24.021 | Train acc: 0.983 | Val acc: 0.981 | Val roc_auc: 0.892 | Training time: 0:04:23\n",
      "Epoch 007: | Loss: 23.197 | Train acc: 0.983 | Val acc: 0.981 | Val roc_auc: 0.870 | Training time: 0:04:24\n",
      "Epoch 008: | Loss: 23.163 | Train acc: 0.983 | Val acc: 0.981 | Val roc_auc: 0.887 | Training time: 0:04:26\n",
      "Epoch     8: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 009: | Loss: 21.605 | Train acc: 0.983 | Val acc: 0.981 | Val roc_auc: 0.902 | Training time: 0:04:25\n",
      "Epoch 010: | Loss: 20.829 | Train acc: 0.983 | Val acc: 0.981 | Val roc_auc: 0.913 | Training time: 0:04:26\n",
      "==================== Fold 2 ====================\n",
      "Loaded pretrained weights for efficientnet-b1\n",
      "Epoch 001: | Loss: 30.536 | Train acc: 0.978 | Val acc: 0.983 | Val roc_auc: 0.868 | Training time: 0:04:28\n",
      "Epoch 002: | Loss: 24.965 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.859 | Training time: 0:04:27\n",
      "Epoch 003: | Loss: 24.681 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.861 | Training time: 0:04:31\n",
      "Epoch     3: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 004: | Loss: 22.014 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.887 | Training time: 0:04:29\n",
      "Epoch 005: | Loss: 20.633 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.896 | Training time: 0:04:31\n",
      "Epoch 006: | Loss: 19.875 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.880 | Training time: 0:04:30\n",
      "Epoch 007: | Loss: 19.311 | Train acc: 0.982 | Val acc: 0.982 | Val roc_auc: 0.891 | Training time: 0:04:30\n",
      "Epoch     7: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 008: | Loss: 16.793 | Train acc: 0.983 | Val acc: 0.982 | Val roc_auc: 0.895 | Training time: 0:04:31\n",
      "Early stopping. Best Val roc_auc: 0.896\n",
      "==================== Fold 3 ====================\n",
      "Loaded pretrained weights for efficientnet-b1\n",
      "Epoch 001: | Loss: 29.718 | Train acc: 0.979 | Val acc: 0.983 | Val roc_auc: 0.807 | Training time: 0:04:27\n",
      "Epoch 002: | Loss: 25.550 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.651 | Training time: 0:04:27\n",
      "Epoch 003: | Loss: 25.154 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.823 | Training time: 0:04:27\n",
      "Epoch 004: | Loss: 24.122 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.847 | Training time: 0:04:27\n",
      "Epoch 005: | Loss: 23.990 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.820 | Training time: 0:04:27\n",
      "Epoch 006: | Loss: 23.218 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.711 | Training time: 0:04:27\n",
      "Epoch     6: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 007: | Loss: 22.280 | Train acc: 0.982 | Val acc: 0.982 | Val roc_auc: 0.867 | Training time: 0:04:28\n",
      "Epoch 008: | Loss: 20.983 | Train acc: 0.982 | Val acc: 0.982 | Val roc_auc: 0.868 | Training time: 0:04:27\n",
      "Epoch 009: | Loss: 20.653 | Train acc: 0.982 | Val acc: 0.982 | Val roc_auc: 0.885 | Training time: 0:04:26\n",
      "Epoch 010: | Loss: 19.547 | Train acc: 0.982 | Val acc: 0.982 | Val roc_auc: 0.877 | Training time: 0:04:26\n",
      "==================== Fold 4 ====================\n",
      "Loaded pretrained weights for efficientnet-b1\n",
      "Epoch 001: | Loss: 30.214 | Train acc: 0.980 | Val acc: 0.983 | Val roc_auc: 0.872 | Training time: 0:04:26\n",
      "Epoch 002: | Loss: 25.578 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.864 | Training time: 0:04:27\n",
      "Epoch 003: | Loss: 25.640 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.869 | Training time: 0:04:28\n",
      "Epoch     3: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 004: | Loss: 23.009 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.876 | Training time: 0:04:26\n",
      "Epoch 005: | Loss: 22.449 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.905 | Training time: 0:04:27\n",
      "Epoch 006: | Loss: 21.950 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.908 | Training time: 0:04:24\n",
      "Epoch 007: | Loss: 21.337 | Train acc: 0.983 | Val acc: 0.983 | Val roc_auc: 0.908 | Training time: 0:04:24\n",
      "Epoch 008: | Loss: 20.750 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.913 | Training time: 0:04:25\n",
      "Epoch 009: | Loss: 20.622 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.893 | Training time: 0:04:25\n",
      "Epoch 010: | Loss: 18.759 | Train acc: 0.983 | Val acc: 0.983 | Val roc_auc: 0.894 | Training time: 0:04:22\n",
      "Epoch    10: reducing learning rate of group 0 to 4.0000e-05.\n",
      "==================== Fold 5 ====================\n",
      "Loaded pretrained weights for efficientnet-b1\n",
      "Epoch 001: | Loss: 29.959 | Train acc: 0.980 | Val acc: 0.983 | Val roc_auc: 0.819 | Training time: 0:04:23\n",
      "Epoch 002: | Loss: 25.067 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.840 | Training time: 0:04:21\n",
      "Epoch 003: | Loss: 24.114 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.820 | Training time: 0:04:22\n",
      "Epoch 004: | Loss: 23.716 | Train acc: 0.982 | Val acc: 0.982 | Val roc_auc: 0.764 | Training time: 0:04:22\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 005: | Loss: 21.851 | Train acc: 0.982 | Val acc: 0.982 | Val roc_auc: 0.853 | Training time: 0:04:24\n",
      "Epoch 006: | Loss: 20.690 | Train acc: 0.982 | Val acc: 0.982 | Val roc_auc: 0.847 | Training time: 0:04:28\n",
      "Epoch 007: | Loss: 19.622 | Train acc: 0.982 | Val acc: 0.982 | Val roc_auc: 0.848 | Training time: 0:04:23\n",
      "Epoch     7: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 008: | Loss: 18.218 | Train acc: 0.983 | Val acc: 0.982 | Val roc_auc: 0.854 | Training time: 0:04:26\n",
      "Epoch 009: | Loss: 17.490 | Train acc: 0.983 | Val acc: 0.982 | Val roc_auc: 0.860 | Training time: 0:04:24\n",
      "Epoch 010: | Loss: 17.265 | Train acc: 0.983 | Val acc: 0.982 | Val roc_auc: 0.860 | Training time: 0:04:24\n"
     ]
    }
   ],
   "source": [
    "epochs = 10  # Number of epochs to run\n",
    "es_patience = 3  # Early Stopping Patience\n",
    "TTA = 3  # Test Time Augmentation rounds\n",
    "\n",
    "oof = np.zeros((len(X_train), 1))  # Out Of Fold predictions\n",
    "preds = torch.zeros(\n",
    "    (len(test), 1), dtype=torch.float32, device=device\n",
    ")  # Predictions for test test\n",
    "\n",
    "# skf = KFold(n_splits=5, shuffle=True, random_state=47)\n",
    "for fold, (train_idx, val_idx) in enumerate(\n",
    "    skf.split(\n",
    "        X=np.zeros(len(X_train)),\n",
    "        y=y_train,\n",
    "        groups=X_train[\"patient_id\"].tolist(),\n",
    "    ),\n",
    "    1,\n",
    "):\n",
    "    print(\"=\" * 20, \"Fold\", fold, \"=\" * 20)\n",
    "\n",
    "    model_path = f\"model_{fold}.pth\"  # Path and filename to save model to\n",
    "    best_val = 0  # Best validation score within this fold\n",
    "    patience = es_patience  # Current patience counter\n",
    "    arch = EfficientNet.from_pretrained(\"efficientnet-b1\")\n",
    "    model = Net(\n",
    "        arch=arch, n_meta_features=len(meta_features)\n",
    "    )  # New model for each fold\n",
    "    model = model.to(device)\n",
    "\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer=optim, mode=\"max\", patience=1, verbose=True, factor=0.2\n",
    "    )\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    train = MelanomaDataset(\n",
    "        df=X_train.iloc[train_idx].reset_index(drop=True),\n",
    "        imfolder=\"/kaggle/input/melanoma-external-malignant-256/train/train/\",\n",
    "        train=True,\n",
    "        transforms=train_transform,\n",
    "        meta_features=meta_features,\n",
    "    )\n",
    "    val = MelanomaDataset(\n",
    "        df=X_train.iloc[val_idx].reset_index(drop=True),\n",
    "        imfolder=\"/kaggle/input/melanoma-external-malignant-256/train/train/\",\n",
    "        train=True,\n",
    "        transforms=test_transform,\n",
    "        meta_features=meta_features,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(dataset=train, batch_size=64, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(dataset=val, batch_size=16, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(dataset=test, batch_size=16, shuffle=False, num_workers=2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        correct = 0\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x[0] = torch.tensor(x[0], device=device, dtype=torch.float32)\n",
    "            x[1] = torch.tensor(x[1], device=device, dtype=torch.float32)\n",
    "            y = torch.tensor(y, device=device, dtype=torch.float32)\n",
    "            optim.zero_grad()\n",
    "            z = model(x)\n",
    "            loss = criterion(z, y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            pred = torch.round(torch.sigmoid(z))\n",
    "            correct += (pred.cpu() == y.cpu().unsqueeze(1)).sum().item()\n",
    "            epoch_loss += loss.item()\n",
    "        train_acc = correct / len(train_idx)\n",
    "\n",
    "        model.eval()\n",
    "        val_preds = torch.zeros((len(val_idx), 1), dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            # Predicting on validation set\n",
    "            for j, (x_val, y_val) in enumerate(val_loader):\n",
    "                x_val[0] = torch.tensor(x_val[0], device=device, dtype=torch.float32)\n",
    "                x_val[1] = torch.tensor(x_val[1], device=device, dtype=torch.float32)\n",
    "                y_val = torch.tensor(y_val, device=device, dtype=torch.float32)\n",
    "                z_val = model(x_val)\n",
    "                val_pred = torch.sigmoid(z_val)\n",
    "                val_preds[\n",
    "                    j * val_loader.batch_size : j * val_loader.batch_size\n",
    "                    + x_val[0].shape[0]\n",
    "                ] = val_pred\n",
    "            val_acc = accuracy_score(\n",
    "                X_train.iloc[val_idx][\"target\"].values, torch.round(val_preds.cpu())\n",
    "            )\n",
    "            val_roc = roc_auc_score(\n",
    "                X_train.iloc[val_idx][\"target\"].values, val_preds.cpu()\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                \"Epoch {:03}: | Loss: {:.3f} | Train acc: {:.3f} | Val acc: {:.3f} | Val roc_auc: {:.3f} | Training time: {}\".format(\n",
    "                    epoch + 1,\n",
    "                    epoch_loss,\n",
    "                    train_acc,\n",
    "                    val_acc,\n",
    "                    val_roc,\n",
    "                    str(datetime.timedelta(seconds=time.time() - start_time))[:7],\n",
    "                )\n",
    "            )\n",
    "\n",
    "            scheduler.step(val_roc)\n",
    "\n",
    "            if val_roc >= best_val:\n",
    "                best_val = val_roc\n",
    "                patience = es_patience\n",
    "                torch.save(model, model_path)\n",
    "            else:\n",
    "                patience -= 1\n",
    "                if patience == 0:\n",
    "                    print(\"Early stopping. Best Val roc_auc: {:.3f}\".format(best_val))\n",
    "                    break\n",
    "\n",
    "    model = torch.load(model_path)\n",
    "    model.eval()\n",
    "    val_preds = torch.zeros((len(val_idx), 1), dtype=torch.float32, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j, (x_val, y_val) in enumerate(val_loader):\n",
    "            x_val[0] = torch.tensor(x_val[0], device=device, dtype=torch.float32)\n",
    "            x_val[1] = torch.tensor(x_val[1], device=device, dtype=torch.float32)\n",
    "            y_val = torch.tensor(y_val, device=device, dtype=torch.float32)\n",
    "            z_val = model(x_val)\n",
    "            val_pred = torch.sigmoid(z_val)\n",
    "            val_preds[\n",
    "                j * val_loader.batch_size : j * val_loader.batch_size\n",
    "                + x_val[0].shape[0]\n",
    "            ] = val_pred\n",
    "        oof[val_idx] = val_preds.cpu().numpy()\n",
    "\n",
    "        tta_preds = torch.zeros((len(test), 1), dtype=torch.float32, device=device)\n",
    "        for _ in range(TTA):\n",
    "            for i, x_test in enumerate(test_loader):\n",
    "                x_test[0] = torch.tensor(x_test[0], device=device, dtype=torch.float32)\n",
    "                x_test[1] = torch.tensor(x_test[1], device=device, dtype=torch.float32)\n",
    "                z_test = model(x_test)\n",
    "                z_test = torch.sigmoid(z_test)\n",
    "                tta_preds[\n",
    "                    i * test_loader.batch_size : i * test_loader.batch_size\n",
    "                    + x_test[0].shape[0]\n",
    "                ] += z_test\n",
    "        preds += tta_preds / TTA\n",
    "\n",
    "preds /= skf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T10:36:31.608375Z",
     "iopub.status.busy": "2021-06-01T10:36:31.608003Z",
     "iopub.status.idle": "2021-06-01T10:36:31.627120Z",
     "shell.execute_reply": "2021-06-01T10:36:31.626317Z",
     "shell.execute_reply.started": "2021-06-01T10:36:31.608332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF: 0.88075\n",
      "Test Score: 0.90292\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF: {roc_auc_score(y_train, oof):.5f}\")\n",
    "print(f\"Test Score: {roc_auc_score(y_test.values, preds.cpu().numpy().flatten()):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T10:36:31.629209Z",
     "iopub.status.busy": "2021-06-01T10:36:31.628712Z",
     "iopub.status.idle": "2021-06-01T10:36:31.761488Z",
     "shell.execute_reply": "2021-06-01T10:36:31.760461Z",
     "shell.execute_reply.started": "2021-06-01T10:36:31.629158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.kdeplot(\n",
    "    pd.Series(\n",
    "        preds.cpu()\n",
    "        .numpy()\n",
    "        .reshape(\n",
    "            -1,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T10:36:31.763426Z",
     "iopub.status.busy": "2021-06-01T10:36:31.763030Z",
     "iopub.status.idle": "2021-06-01T10:36:31.932321Z",
     "shell.execute_reply": "2021-06-01T10:36:31.931556Z",
     "shell.execute_reply.started": "2021-06-01T10:36:31.763385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving OOF predictions so stacking would be easier\n",
    "pd.Series(\n",
    "    oof.reshape(\n",
    "        -1,\n",
    "    )\n",
    ").to_csv(\"oof.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T10:36:31.934680Z",
     "iopub.status.busy": "2021-06-01T10:36:31.934137Z",
     "iopub.status.idle": "2021-06-01T10:36:31.967950Z",
     "shell.execute_reply": "2021-06-01T10:36:31.967326Z",
     "shell.execute_reply.started": "2021-06-01T10:36:31.934642Z"
    }
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(X_test[\"image_name\"])\n",
    "sub[\"target\"] = (\n",
    "    preds.cpu()\n",
    "    .numpy()\n",
    "    .reshape(\n",
    "        -1,\n",
    "    )\n",
    ")\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
