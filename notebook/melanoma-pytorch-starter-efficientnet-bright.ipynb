{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install efficientnet_pytorch torchtoolbox\n!pip install chart_studio","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-06T09:37:02.046430Z","iopub.execute_input":"2021-06-06T09:37:02.046817Z","iopub.status.idle":"2021-06-06T09:37:12.762333Z","shell.execute_reply.started":"2021-06-06T09:37:02.046781Z","shell.execute_reply":"2021-06-06T09:37:12.761491Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: efficientnet_pytorch in /opt/conda/lib/python3.7/site-packages (0.7.1)\nRequirement already satisfied: torchtoolbox in /opt/conda/lib/python3.7/site-packages (0.1.5)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet_pytorch) (1.5.0)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (4.2.0.34)\nRequirement already satisfied: lmdb in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.2.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (0.22.2.post1)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (0.16.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.4.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (4.45.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.18.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.14.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (0.18.2)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torchtoolbox) (0.14.1)\n\u001b[33mWARNING: You are using pip version 20.1; however, version 21.1.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: chart_studio in /opt/conda/lib/python3.7/site-packages (1.1.0)\nRequirement already satisfied: retrying>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from chart_studio) (1.3.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from chart_studio) (2.23.0)\nRequirement already satisfied: plotly in /opt/conda/lib/python3.7/site-packages (from chart_studio) (4.7.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from chart_studio) (1.14.0)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->chart_studio) (1.24.3)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->chart_studio) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->chart_studio) (2.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->chart_studio) (2020.4.5.1)\n\u001b[33mWARNING: You are using pip version 20.1; however, version 21.1.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torchtoolbox.transform as transforms\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nimport pandas as pd\nimport numpy as np\nimport gc\nimport os\nimport cv2\nimport time\nimport datetime\nimport warnings\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom efficientnet_pytorch import EfficientNet\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-06T09:37:12.764114Z","iopub.execute_input":"2021-06-06T09:37:12.764462Z","iopub.status.idle":"2021-06-06T09:37:13.731027Z","shell.execute_reply.started":"2021-06-06T09:37:12.764433Z","shell.execute_reply":"2021-06-06T09:37:13.730297Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"warnings.simplefilter('ignore')\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(47)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:37:25.793723Z","iopub.execute_input":"2021-06-06T09:37:25.794055Z","iopub.status.idle":"2021-06-06T09:37:25.801937Z","shell.execute_reply.started":"2021-06-06T09:37:25.794024Z","shell.execute_reply":"2021-06-06T09:37:25.801003Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:37:29.405824Z","iopub.execute_input":"2021-06-06T09:37:29.406149Z","iopub.status.idle":"2021-06-06T09:37:29.431565Z","shell.execute_reply.started":"2021-06-06T09:37:29.406118Z","shell.execute_reply":"2021-06-06T09:37:29.430518Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class MelanomaDataset(Dataset):\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        imfolder: str,\n        train: bool = True,\n        transforms=None,\n        meta_features=None,\n    ):\n        self.df = df\n        self.imfolder = imfolder\n        self.transforms = transforms\n        self.train = train\n        self.meta_features = meta_features\n\n    def __getitem__(self, index):\n        im_path = os.path.join(\n            self.imfolder, self.df.iloc[index][\"image_name\"] + \".jpg\"\n        )\n        x = cv2.imread(im_path)\n        x = cv2.cvtColor(x, cv2.COLOR_RGB2BGR)\n    \n        val = 50\n        array = np.full(x.shape, (val, val, val), dtype=np.uint8)\n        image = cv2.add(x, array)\n        meta = np.array(\n            self.df.iloc[index][self.meta_features].values, dtype=np.float32\n        )\n\n        if self.transforms:\n            x = self.transforms(x)\n\n        if self.train:\n            y = self.df.iloc[index][\"target\"]\n            return (x, meta), y\n        else:\n            return (x, meta)\n\n    def __len__(self):\n        return len(self.df)\n\n\nclass Net(nn.Module):\n    def __init__(self, arch, n_meta_features: int):\n        super(Net, self).__init__()\n        self.arch = arch\n        if \"ResNet\" in str(arch.__class__):\n            self.arch.fc = nn.Linear(in_features=512, out_features=500, bias=True)\n        if \"EfficientNet\" in str(arch.__class__):\n            self.arch._fc = nn.Linear(in_features=1280, out_features=500, bias=True)\n        self.meta = nn.Sequential(\n            nn.Linear(n_meta_features, 500),\n            nn.BatchNorm1d(500),\n            nn.ReLU(),\n            nn.Dropout(p=0.2),\n            nn.Linear(500, 250),\n            nn.BatchNorm1d(250),\n            nn.ReLU(),\n            nn.Dropout(p=0.2),\n        )\n        self.ouput = nn.Linear(500 + 250, 1)\n\n    def forward(self, inputs):\n        x, meta = inputs\n        cnn_features = self.arch(x)\n        meta_features = self.meta(meta)\n        features = torch.cat((cnn_features, meta_features), dim=1)\n        output = self.ouput(features)\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:37:38.321354Z","iopub.execute_input":"2021-06-06T09:37:38.321688Z","iopub.status.idle":"2021-06-06T09:37:38.341182Z","shell.execute_reply.started":"2021-06-06T09:37:38.321657Z","shell.execute_reply":"2021-06-06T09:37:38.340009Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class AdvancedHairAugmentation:\n    def __init__(self, hairs: int = 5, hairs_folder: str = \"\"):\n        self.hairs = hairs\n        self.hairs_folder = hairs_folder\n\n    def __call__(self, img):\n        n_hairs = random.randint(0, self.hairs)\n\n        if not n_hairs:\n            return img\n\n        height, width, _ = img.shape  # target image width and height\n        hair_images = [im for im in os.listdir(self.hairs_folder) if \"png\" in im]\n\n        for _ in range(n_hairs):\n            hair = cv2.imread(\n                os.path.join(self.hairs_folder, random.choice(hair_images))\n            )\n            hair = cv2.flip(hair, random.choice([-1, 0, 1]))\n            hair = cv2.rotate(hair, random.choice([0, 1, 2]))\n\n            h_height, h_width, _ = hair.shape  # hair image width and height\n            roi_ho = random.randint(0, img.shape[0] - hair.shape[0])\n            roi_wo = random.randint(0, img.shape[1] - hair.shape[1])\n            roi = img[roi_ho : roi_ho + h_height, roi_wo : roi_wo + h_width]\n\n            # Creating a mask and inverse mask\n            img2gray = cv2.cvtColor(hair, cv2.COLOR_BGR2GRAY)\n            ret, mask = cv2.threshold(img2gray, 10, 255, cv2.THRESH_BINARY)\n            mask_inv = cv2.bitwise_not(mask)\n\n            # Now black-out the area of hair in ROI\n            img_bg = cv2.bitwise_and(roi, roi, mask=mask_inv)\n\n            # Take only region of hair from hair image.\n            hair_fg = cv2.bitwise_and(hair, hair, mask=mask)\n\n            # Put hair in ROI and modify the target image\n            dst = cv2.add(img_bg, hair_fg)\n\n            img[roi_ho : roi_ho + h_height, roi_wo : roi_wo + h_width] = dst\n\n        return img\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}(hairs={self.hairs}, hairs_folder=\"{self.hairs_folder}\")'\n","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:37:39.462225Z","iopub.execute_input":"2021-06-06T09:37:39.462544Z","iopub.status.idle":"2021-06-06T09:37:39.476464Z","shell.execute_reply.started":"2021-06-06T09:37:39.462509Z","shell.execute_reply":"2021-06-06T09:37:39.475606Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class DrawHair:\n    def __init__(self, hairs: int = 4, width: tuple = (1, 2)):\n        self.hairs = hairs\n        self.width = width\n\n    def __call__(self, img):\n        if not self.hairs:\n            return img\n\n        width, height, _ = img.shape\n\n        for _ in range(random.randint(0, self.hairs)):\n            # The origin point of the line will always be at the top half of the image\n            origin = (random.randint(0, width), random.randint(0, height // 2))\n            # The end of the line\n            end = (random.randint(0, width), random.randint(0, height))\n            color = (0, 0, 0)  # color of the hair. Black.\n            cv2.line(\n                img, origin, end, color, random.randint(self.width[0], self.width[1])\n            )\n\n        return img\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}(hairs={self.hairs}, width={self.width})\"","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:37:41.589667Z","iopub.execute_input":"2021-06-06T09:37:41.590101Z","iopub.status.idle":"2021-06-06T09:37:41.600239Z","shell.execute_reply.started":"2021-06-06T09:37:41.590064Z","shell.execute_reply":"2021-06-06T09:37:41.599036Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Microscope:\n    def __init__(self, p: float = 0.5):\n        self.p = p\n\n    def __call__(self, img):\n        if random.random() < self.p:\n            circle = cv2.circle(\n                (np.ones(img.shape) * 255).astype(np.uint8),  # image placeholder\n                (img.shape[0] // 2, img.shape[1] // 2),  # center point of circle\n                random.randint(img.shape[0] // 2 - 3, img.shape[0] // 2 + 15),  # radius\n                (0, 0, 0),  # color\n                -1,\n            )\n\n            mask = circle - 255\n            img = np.multiply(img, mask)\n\n        return img\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}(p={self.p})\"","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:37:46.053312Z","iopub.execute_input":"2021-06-06T09:37:46.053656Z","iopub.status.idle":"2021-06-06T09:37:46.062865Z","shell.execute_reply.started":"2021-06-06T09:37:46.053624Z","shell.execute_reply":"2021-06-06T09:37:46.061756Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    AdvancedHairAugmentation(hairs_folder=\"/kaggle/input/melanoma-hairs\"),\n    transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    Microscope(p=0.5),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n])\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:38:19.206697Z","iopub.execute_input":"2021-06-06T09:38:19.207037Z","iopub.status.idle":"2021-06-06T09:38:19.214508Z","shell.execute_reply.started":"2021-06-06T09:38:19.207007Z","shell.execute_reply":"2021-06-06T09:38:19.213357Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"arch = EfficientNet.from_pretrained('efficientnet-b1')","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:38:19.982883Z","iopub.execute_input":"2021-06-06T09:38:19.983205Z","iopub.status.idle":"2021-06-06T09:38:20.151167Z","shell.execute_reply.started":"2021-06-06T09:38:19.983174Z","shell.execute_reply":"2021-06-06T09:38:20.150292Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Loaded pretrained weights for efficientnet-b1\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/jpeg-melanoma-256x256/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:38:26.095297Z","iopub.execute_input":"2021-06-06T09:38:26.095647Z","iopub.status.idle":"2021-06-06T09:38:26.152185Z","shell.execute_reply.started":"2021-06-06T09:38:26.095615Z","shell.execute_reply":"2021-06-06T09:38:26.151465Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# One-hot encoding of anatom_site_general_challenge feature\ndummies = pd.get_dummies(train_df['anatom_site_general_challenge'], dummy_na=True, dtype=np.uint8, prefix='site')\ntrain_df = pd.concat([train_df, dummies.iloc[:train_df.shape[0]]], axis=1)\n\n# Sex features\ntrain_df['sex'] = train_df['sex'].map({'male': 1, 'female': 0})\ntrain_df['sex'] = train_df['sex'].fillna(-1)\n\n# Age features\ntrain_df['age_approx'] /= train_df['age_approx'].max()\ntrain_df['age_approx'] = train_df['age_approx'].fillna(0)\ntrain_df['patient_id'] = train_df['patient_id'].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:38:26.892278Z","iopub.execute_input":"2021-06-06T09:38:26.892622Z","iopub.status.idle":"2021-06-06T09:38:26.926687Z","shell.execute_reply.started":"2021-06-06T09:38:26.892591Z","shell.execute_reply":"2021-06-06T09:38:26.925955Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"meta_features = ['sex', 'age_approx'] + [col for col in train_df.columns if 'site_' in col]\nmeta_features.remove('anatom_site_general_challenge')","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:38:28.012739Z","iopub.execute_input":"2021-06-06T09:38:28.013059Z","iopub.status.idle":"2021-06-06T09:38:28.017974Z","shell.execute_reply.started":"2021-06-06T09:38:28.013029Z","shell.execute_reply":"2021-06-06T09:38:28.016791Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = train_df.drop(\"target\", axis=1)\ny = train_df[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:38:28.899895Z","iopub.execute_input":"2021-06-06T09:38:28.900229Z","iopub.status.idle":"2021-06-06T09:38:28.918926Z","shell.execute_reply.started":"2021-06-06T09:38:28.900183Z","shell.execute_reply":"2021-06-06T09:38:28.918232Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"X_train[\"target\"] = y_train","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:38:32.492016Z","iopub.execute_input":"2021-06-06T09:38:32.492384Z","iopub.status.idle":"2021-06-06T09:38:32.497993Z","shell.execute_reply.started":"2021-06-06T09:38:32.492345Z","shell.execute_reply":"2021-06-06T09:38:32.496862Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"test = MelanomaDataset(\n    df=X_test.reset_index(drop=True),\n    imfolder=\"/kaggle/input/melanoma-external-malignant-256/train/train/\",\n    train=False,\n    transforms=train_transform,  # For TTA\n    meta_features=meta_features,\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:38:38.143709Z","iopub.execute_input":"2021-06-06T09:38:38.144037Z","iopub.status.idle":"2021-06-06T09:38:38.151173Z","shell.execute_reply.started":"2021-06-06T09:38:38.144005Z","shell.execute_reply":"2021-06-06T09:38:38.150310Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"skf = GroupKFold(n_splits=5)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:38:39.892733Z","iopub.execute_input":"2021-06-06T09:38:39.893065Z","iopub.status.idle":"2021-06-06T09:38:39.897396Z","shell.execute_reply.started":"2021-06-06T09:38:39.893033Z","shell.execute_reply":"2021-06-06T09:38:39.896312Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"epochs = 5  # Number of epochs to run\nes_patience = 3 # Early Stopping Patience\nTTA = 3  # Test Time Augmentation rounds\n\noof = np.zeros((len(X_train), 1))  # Out Of Fold predictions\npreds = torch.zeros(\n    (len(test), 1), dtype=torch.float32, device=device\n)  # Predictions for test test\n\n# skf = KFold(n_splits=5, shuffle=True, random_state=47)\nfor fold, (train_idx, val_idx) in enumerate(\n    skf.split(\n        X=np.zeros(len(X_train)),\n        y=y_train,\n        groups=X_train[\"patient_id\"].tolist(),\n    ),\n    1,\n):\n    print(\"=\" * 20, \"Fold\", fold, \"=\" * 20)\n\n    model_path = f\"model_{fold}.pth\"  # Path and filename to save model to\n    best_val = 0  # Best validation score within this fold\n    patience = es_patience  # Current patience counter\n    arch = EfficientNet.from_pretrained(\"efficientnet-b1\")\n    model = Net(\n        arch=arch, n_meta_features=len(meta_features)\n    )  # New model for each fold\n    model = model.to(device)\n\n    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n    scheduler = ReduceLROnPlateau(\n        optimizer=optim, mode=\"max\", patience=1, verbose=True, factor=0.2\n    )\n    criterion = nn.BCEWithLogitsLoss()\n\n    train = MelanomaDataset(\n        df=X_train.iloc[train_idx].reset_index(drop=True),\n        imfolder=\"/kaggle/input/melanoma-external-malignant-256/train/train/\",\n        train=True,\n        transforms=train_transform,\n        meta_features=meta_features,\n    )\n    val = MelanomaDataset(\n        df=X_train.iloc[val_idx].reset_index(drop=True),\n        imfolder=\"/kaggle/input/melanoma-external-malignant-256/train/train/\",\n        train=True,\n        transforms=test_transform,\n        meta_features=meta_features,\n    )\n\n    train_loader = DataLoader(dataset=train, batch_size=64, shuffle=True, num_workers=2)\n    val_loader = DataLoader(dataset=val, batch_size=16, shuffle=False, num_workers=2)\n    test_loader = DataLoader(dataset=test, batch_size=16, shuffle=False, num_workers=2)\n\n    for epoch in range(epochs):\n        start_time = time.time()\n        correct = 0\n        epoch_loss = 0\n        model.train()\n\n        for x, y in train_loader:\n            x[0] = torch.tensor(x[0], device=device, dtype=torch.float32)\n            x[1] = torch.tensor(x[1], device=device, dtype=torch.float32)\n            y = torch.tensor(y, device=device, dtype=torch.float32)\n            optim.zero_grad()\n            z = model(x)\n            loss = criterion(z, y.unsqueeze(1))\n            loss.backward()\n            optim.step()\n            pred = torch.round(\n                torch.sigmoid(z)\n            )\n            correct += (\n                (pred.cpu() == y.cpu().unsqueeze(1)).sum().item()\n            )\n            epoch_loss += loss.item()\n        train_acc = correct / len(train_idx)\n\n        model.eval()\n        val_preds = torch.zeros((len(val_idx), 1), dtype=torch.float32, device=device)\n        with torch.no_grad():\n            # Predicting on validation set\n            for j, (x_val, y_val) in enumerate(val_loader):\n                x_val[0] = torch.tensor(x_val[0], device=device, dtype=torch.float32)\n                x_val[1] = torch.tensor(x_val[1], device=device, dtype=torch.float32)\n                y_val = torch.tensor(y_val, device=device, dtype=torch.float32)\n                z_val = model(x_val)\n                val_pred = torch.sigmoid(z_val)\n                val_preds[\n                    j * val_loader.batch_size : j * val_loader.batch_size\n                    + x_val[0].shape[0]\n                ] = val_pred\n            val_acc = accuracy_score(\n                X_train.iloc[val_idx][\"target\"].values, torch.round(val_preds.cpu())\n            )\n            val_roc = roc_auc_score(\n                X_train.iloc[val_idx][\"target\"].values, val_preds.cpu()\n            )\n\n            print(\n                \"Epoch {:03}: | Loss: {:.3f} | Train acc: {:.3f} | Val acc: {:.3f} | Val roc_auc: {:.3f} | Training time: {}\".format(\n                    epoch + 1,\n                    epoch_loss,\n                    train_acc,\n                    val_acc,\n                    val_roc,\n                    str(datetime.timedelta(seconds=time.time() - start_time))[:7],\n                )\n            )\n\n            scheduler.step(val_roc)\n\n            if val_roc >= best_val:\n                best_val = val_roc\n                patience = es_patience\n                torch.save(model, model_path)\n            else:\n                patience -= 1\n                if patience == 0:\n                    print(\"Early stopping. Best Val roc_auc: {:.3f}\".format(best_val))\n                    break\n\n    model = torch.load(model_path)\n    model.eval()\n    val_preds = torch.zeros((len(val_idx), 1), dtype=torch.float32, device=device)\n\n    with torch.no_grad():\n        for j, (x_val, y_val) in enumerate(val_loader):\n            x_val[0] = torch.tensor(x_val[0], device=device, dtype=torch.float32)\n            x_val[1] = torch.tensor(x_val[1], device=device, dtype=torch.float32)\n            y_val = torch.tensor(y_val, device=device, dtype=torch.float32)\n            z_val = model(x_val)\n            val_pred = torch.sigmoid(z_val)\n            val_preds[\n                j * val_loader.batch_size : j * val_loader.batch_size\n                + x_val[0].shape[0]\n            ] = val_pred\n        oof[val_idx] = val_preds.cpu().numpy()\n\n        tta_preds = torch.zeros((len(test), 1), dtype=torch.float32, device=device)\n        for _ in range(TTA):\n            for i, x_test in enumerate(test_loader):\n                x_test[0] = torch.tensor(x_test[0], device=device, dtype=torch.float32)\n                x_test[1] = torch.tensor(x_test[1], device=device, dtype=torch.float32)\n                z_test = model(x_test)\n                z_test = torch.sigmoid(z_test)\n                tta_preds[\n                    i * test_loader.batch_size : i * test_loader.batch_size\n                    + x_test[0].shape[0]\n                ] += z_test\n        preds += tta_preds / TTA\n\npreds /= skf.n_splits","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:38:40.802046Z","iopub.execute_input":"2021-06-06T09:38:40.802410Z","iopub.status.idle":"2021-06-06T11:51:41.360399Z","shell.execute_reply.started":"2021-06-06T09:38:40.802377Z","shell.execute_reply":"2021-06-06T11:51:41.359423Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"==================== Fold 1 ====================\nLoaded pretrained weights for efficientnet-b1\nEpoch 001: | Loss: 29.181 | Train acc: 0.981 | Val acc: 0.981 | Val roc_auc: 0.798 | Training time: 0:05:31\nEpoch 002: | Loss: 24.932 | Train acc: 0.983 | Val acc: 0.981 | Val roc_auc: 0.822 | Training time: 0:04:31\nEpoch 003: | Loss: 24.606 | Train acc: 0.983 | Val acc: 0.981 | Val roc_auc: 0.766 | Training time: 0:04:31\nEpoch 004: | Loss: 24.551 | Train acc: 0.983 | Val acc: 0.981 | Val roc_auc: 0.863 | Training time: 0:04:32\nEpoch 005: | Loss: 23.637 | Train acc: 0.983 | Val acc: 0.981 | Val roc_auc: 0.832 | Training time: 0:04:32\n==================== Fold 2 ====================\nLoaded pretrained weights for efficientnet-b1\nEpoch 001: | Loss: 30.553 | Train acc: 0.979 | Val acc: 0.983 | Val roc_auc: 0.819 | Training time: 0:04:35\nEpoch 002: | Loss: 25.589 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.850 | Training time: 0:04:32\nEpoch 003: | Loss: 24.832 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.801 | Training time: 0:04:33\nEpoch 004: | Loss: 24.795 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.872 | Training time: 0:04:32\nEpoch 005: | Loss: 24.266 | Train acc: 0.982 | Val acc: 0.881 | Val roc_auc: 0.850 | Training time: 0:04:33\n==================== Fold 3 ====================\nLoaded pretrained weights for efficientnet-b1\nEpoch 001: | Loss: 29.256 | Train acc: 0.980 | Val acc: 0.983 | Val roc_auc: 0.678 | Training time: 0:04:33\nEpoch 002: | Loss: 25.385 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.840 | Training time: 0:04:34\nEpoch 003: | Loss: 24.985 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.859 | Training time: 0:04:33\nEpoch 004: | Loss: 25.528 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.797 | Training time: 0:04:35\nEpoch 005: | Loss: 25.039 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.821 | Training time: 0:04:34\nEpoch     5: reducing learning rate of group 0 to 2.0000e-04.\n==================== Fold 4 ====================\nLoaded pretrained weights for efficientnet-b1\nEpoch 001: | Loss: 30.169 | Train acc: 0.980 | Val acc: 0.983 | Val roc_auc: 0.863 | Training time: 0:04:31\nEpoch 002: | Loss: 25.616 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.855 | Training time: 0:04:31\nEpoch 003: | Loss: 25.032 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.846 | Training time: 0:04:30\nEpoch     3: reducing learning rate of group 0 to 2.0000e-04.\nEpoch 004: | Loss: 23.225 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.886 | Training time: 0:04:32\nEpoch 005: | Loss: 21.845 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.896 | Training time: 0:04:30\n==================== Fold 5 ====================\nLoaded pretrained weights for efficientnet-b1\nEpoch 001: | Loss: 29.568 | Train acc: 0.980 | Val acc: 0.983 | Val roc_auc: 0.777 | Training time: 0:04:31\nEpoch 002: | Loss: 24.537 | Train acc: 0.982 | Val acc: 0.983 | Val roc_auc: 0.825 | Training time: 0:04:31\nEpoch 003: | Loss: 23.707 | Train acc: 0.982 | Val acc: 0.912 | Val roc_auc: 0.833 | Training time: 0:04:31\nEpoch 004: | Loss: 23.841 | Train acc: 0.982 | Val acc: 0.978 | Val roc_auc: 0.842 | Training time: 0:04:30\nEpoch 005: | Loss: 23.086 | Train acc: 0.982 | Val acc: 0.982 | Val roc_auc: 0.817 | Training time: 0:04:31\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f'OOF: {roc_auc_score(y_train, oof):.5f}')\nprint(f\"Test Score: {roc_auc_score(y_test.values, preds.cpu().numpy().flatten()):.5f}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:51:41.362777Z","iopub.execute_input":"2021-06-06T11:51:41.363158Z","iopub.status.idle":"2021-06-06T11:51:41.381187Z","shell.execute_reply.started":"2021-06-06T11:51:41.363116Z","shell.execute_reply":"2021-06-06T11:51:41.380361Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"OOF: 0.85352\nTest Score: 0.86839\n","output_type":"stream"}]},{"cell_type":"code","source":"sns.kdeplot(pd.Series(preds.cpu().numpy().reshape(-1,)));","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:51:41.382753Z","iopub.execute_input":"2021-06-06T11:51:41.383111Z","iopub.status.idle":"2021-06-06T11:51:41.546424Z","shell.execute_reply.started":"2021-06-06T11:51:41.383075Z","shell.execute_reply":"2021-06-06T11:51:41.545528Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"# Saving OOF predictions so stacking would be easier\npd.Series(oof.reshape(-1,)).to_csv('oof.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:51:41.548018Z","iopub.execute_input":"2021-06-06T11:51:41.548377Z","iopub.status.idle":"2021-06-06T11:51:41.849652Z","shell.execute_reply.started":"2021-06-06T11:51:41.548341Z","shell.execute_reply":"2021-06-06T11:51:41.848949Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame(X_test[\"image_name\"])\nsub['target'] = preds.cpu().numpy().reshape(-1,)\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:51:41.851360Z","iopub.execute_input":"2021-06-06T11:51:41.851616Z","iopub.status.idle":"2021-06-06T11:51:41.883353Z","shell.execute_reply.started":"2021-06-06T11:51:41.851591Z","shell.execute_reply":"2021-06-06T11:51:41.882578Z"},"trusted":true},"execution_count":22,"outputs":[]}]}